{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "assignment4_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SerasLain/ML_2019/blob/master/assignment4_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkHBOfXjdz8m",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 4: Named entity recognition\n",
        "\n",
        "Create a model for Named Entity Recognition for dataset CoNLL 2002.  \n",
        "Your quality metric = f1_macro\n",
        "\n",
        "In your solution you should use: RandomForest, Gradient Boosting (xgboost, lightgbm, catboost)   \n",
        "Tutorials:  \n",
        "1. https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide\n",
        "1. https://github.com/catboost/tutorials \n",
        "\n",
        "More baselines you beat - better your score\n",
        " \n",
        "baseline 1 [3 points]: 0.0604      random labels  \n",
        "baseline 2 [5 points]: 0.3966      PoS features + logistic regression  \n",
        "baseline 3 [8 points]: 0.8122      word2vec cbow embedding + baseline 2 + svm    \n",
        "\n",
        "[1 point] using feature engineering (creating features not presented in the baselines)\n",
        "\n",
        "! Your results must be reproducible. You should explicitly set all seeds random_states in yout model.  \n",
        "! Remember to use proper training pipeline.  \n",
        "\n",
        "bonus, think about:  \n",
        "1. [1 point] Why did we select f1 score with macro averaging as our classification quality measure? What other metrics are suitable?   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJrKloLJEzzR",
        "colab_type": "text"
      },
      "source": [
        "Мы выбрали f1 score with macro averaging как меру качества классификации, потому что наш датасет несбалансирован и классификация мультиклассовая. Также мы могли бы использовать balanced_accuracy_score (если бы классификация была бинарная, то можно было бы попробовать Matthews correlation coefficient, но это не наш случай)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_XlP1HDdz9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn import model_selection\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import metrics\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "SEED=1337"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDnsSvorfwJy",
        "colab_type": "code",
        "outputId": "5e4bc9c0-cbdf-4990-bc78-787e6378b895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "!wget -O data.zip https://github.com/thedenaas/hse_seminars/blob/master/2019/seminar_6/data.zip?raw=true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-01 18:06:36--  https://github.com/thedenaas/hse_seminars/blob/master/2019/seminar_6/data.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/thedenaas/hse_seminars/raw/master/2019/seminar_6/data.zip [following]\n",
            "--2019-12-01 18:06:37--  https://github.com/thedenaas/hse_seminars/raw/master/2019/seminar_6/data.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/thedenaas/hse_seminars/master/2019/seminar_6/data.zip [following]\n",
            "--2019-12-01 18:06:37--  https://raw.githubusercontent.com/thedenaas/hse_seminars/master/2019/seminar_6/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1064698 (1.0M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   1.01M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-12-01 18:06:39 (12.7 MB/s) - ‘data.zip’ saved [1064698/1064698]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5HbyVTlf8P4",
        "colab_type": "code",
        "outputId": "1e04d393-4140-4402-d8f1-67822b4ca968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!unzip /content/data.zip\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/data.zip\n",
            "replace data/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_A9QM2Idz9O",
        "colab_type": "code",
        "outputId": "81c2d79f-1b5c-43f6-92b4-be047b209c2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "df = pd.read_csv('/content/data/ner_short.csv', index_col=0)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>next-next-pos</th>\n",
              "      <th>next-next-word</th>\n",
              "      <th>next-pos</th>\n",
              "      <th>next-word</th>\n",
              "      <th>pos</th>\n",
              "      <th>prev-pos</th>\n",
              "      <th>prev-prev-pos</th>\n",
              "      <th>prev-prev-word</th>\n",
              "      <th>prev-word</th>\n",
              "      <th>sentence_idx</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NNS</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>IN</td>\n",
              "      <td>of</td>\n",
              "      <td>NNS</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>__START2__</td>\n",
              "      <td>__START2__</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VBP</td>\n",
              "      <td>have</td>\n",
              "      <td>NNS</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>IN</td>\n",
              "      <td>NNS</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>1.0</td>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VBN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBP</td>\n",
              "      <td>have</td>\n",
              "      <td>NNS</td>\n",
              "      <td>IN</td>\n",
              "      <td>NNS</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>1.0</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IN</td>\n",
              "      <td>through</td>\n",
              "      <td>VBN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBP</td>\n",
              "      <td>NNS</td>\n",
              "      <td>IN</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>1.0</td>\n",
              "      <td>have</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NNP</td>\n",
              "      <td>London</td>\n",
              "      <td>IN</td>\n",
              "      <td>through</td>\n",
              "      <td>VBN</td>\n",
              "      <td>VBP</td>\n",
              "      <td>NNS</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>1.0</td>\n",
              "      <td>marched</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  next-next-pos next-next-word next-pos  ... sentence_idx           word tag\n",
              "0           NNS  demonstrators       IN  ...          1.0      Thousands   O\n",
              "1           VBP           have      NNS  ...          1.0             of   O\n",
              "2           VBN        marched      VBP  ...          1.0  demonstrators   O\n",
              "3            IN        through      VBN  ...          1.0           have   O\n",
              "4           NNP         London       IN  ...          1.0        marched   O\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exU1YGfgdz9W",
        "colab_type": "code",
        "outputId": "b6248aaa-01e7-4d48-9955-6414cf1186fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# number of sentences\n",
        "df.sentence_idx.max()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1500.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw1irpjcdz9a",
        "colab_type": "code",
        "outputId": "651f8fae-6463-4837-8a58-233b75679618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# class distribution\n",
        "df.tag.value_counts(normalize=True )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O        0.852828\n",
              "B-geo    0.027604\n",
              "B-gpe    0.020935\n",
              "B-org    0.020247\n",
              "I-per    0.017795\n",
              "B-tim    0.016927\n",
              "B-per    0.015312\n",
              "I-org    0.013937\n",
              "I-geo    0.005383\n",
              "I-tim    0.004247\n",
              "B-art    0.001376\n",
              "I-gpe    0.000837\n",
              "I-art    0.000748\n",
              "B-eve    0.000628\n",
              "I-eve    0.000508\n",
              "B-nat    0.000449\n",
              "I-nat    0.000239\n",
              "Name: tag, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3L1IOBWdz9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentence length\n",
        "tdf = df.set_index('sentence_idx')\n",
        "tdf['length'] = df.groupby('sentence_idx').tag.count()\n",
        "df = tdf.reset_index(drop=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxkPlmu0dz9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode categorial variables\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['pos'] = le.fit_transform(df.pos)\n",
        "df['next-pos'] = le.fit_transform(df['next-pos'])\n",
        "df['next-next-pos'] = le.fit_transform(df['next-next-pos'])\n",
        "df['prev-pos'] = le.fit_transform(df['prev-pos'])\n",
        "df['prev-prev-pos'] = le.fit_transform(df['prev-prev-pos'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyLtO402dz9l",
        "colab_type": "code",
        "outputId": "4cce107f-a01d-41ad-d48d-1dceda1021ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_idx</th>\n",
              "      <th>next-next-pos</th>\n",
              "      <th>next-next-word</th>\n",
              "      <th>next-pos</th>\n",
              "      <th>next-word</th>\n",
              "      <th>pos</th>\n",
              "      <th>prev-pos</th>\n",
              "      <th>prev-prev-pos</th>\n",
              "      <th>prev-prev-word</th>\n",
              "      <th>prev-word</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>9</td>\n",
              "      <td>of</td>\n",
              "      <td>18</td>\n",
              "      <td>39</td>\n",
              "      <td>40</td>\n",
              "      <td>__START2__</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>33</td>\n",
              "      <td>have</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>39</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>32</td>\n",
              "      <td>marched</td>\n",
              "      <td>33</td>\n",
              "      <td>have</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>through</td>\n",
              "      <td>32</td>\n",
              "      <td>marched</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>16</td>\n",
              "      <td>London</td>\n",
              "      <td>9</td>\n",
              "      <td>through</td>\n",
              "      <td>32</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>marched</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_idx  next-next-pos next-next-word  ...           word tag  length\n",
              "0           1.0             18  demonstrators  ...      Thousands   O      48\n",
              "1           1.0             33           have  ...             of   O      48\n",
              "2           1.0             32        marched  ...  demonstrators   O      48\n",
              "3           1.0              9        through  ...           have   O      48\n",
              "4           1.0             16         London  ...        marched   O      48\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIxf24LDdz9p",
        "colab_type": "code",
        "outputId": "b57a43ef-8236-40d4-cd41-2683c87dc154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# splitting\n",
        "y = LabelEncoder().fit_transform(df.tag)\n",
        "\n",
        "df_train, df_test, y_train, y_test = model_selection.train_test_split(df, y, stratify=y, \n",
        "                                                                      test_size=0.25, random_state=SEED, shuffle=True)\n",
        "print('train', df_train.shape[0])\n",
        "print('test', df_test.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 50155\n",
            "test 16719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTtGWr8adz9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some wrappers to work with word2vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from collections import defaultdict\n",
        "\n",
        "   \n",
        "class Word2VecWrapper(TransformerMixin):\n",
        "    def __init__(self, window=5,negative=5, size=100, iter=100, is_cbow=False, random_state=SEED):\n",
        "        self.window_ = window\n",
        "        self.negative_ = negative\n",
        "        self.size_ = size\n",
        "        self.iter_ = iter\n",
        "        self.is_cbow_ = is_cbow\n",
        "        self.w2v = None\n",
        "        self.random_state = random_state\n",
        "        \n",
        "    def get_size(self):\n",
        "        return self.size_\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        X: list of strings\n",
        "        \"\"\"\n",
        "        sentences_list = [x.split() for x in X]\n",
        "        self.w2v = Word2Vec(sentences_list, \n",
        "                            window=self.window_,\n",
        "                            negative=self.negative_, \n",
        "                            size=self.size_, \n",
        "                            iter=self.iter_,\n",
        "                            sg=not self.is_cbow_, seed=self.random_state)\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def has(self, word):\n",
        "        return word in self.w2v\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        X: a list of words\n",
        "        \"\"\"\n",
        "        if self.w2v is None:\n",
        "            raise Exception('model not fitted')\n",
        "        return np.array([self.w2v[w] if w in self.w2v else np.zeros(self.size_) for w in X ])\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKA6CGmNdz9z",
        "colab_type": "code",
        "outputId": "e18b8570-1d01-4767-ee9a-6dcc8e303278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "# here we exploit that word2vec is an unsupervised learning algorithm\n",
        "# so we can train it on the whole dataset (subject to discussion)\n",
        "\n",
        "sentences_list = [x.strip() for x in ' '.join(df.word).split('.')]\n",
        "\n",
        "w2v_cbow = Word2VecWrapper(window=5, negative=5, size=300, iter=300, is_cbow=True, random_state=SEED)\n",
        "w2v_cbow.fit(sentences_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 47.9 s, sys: 482 ms, total: 48.4 s\n",
            "Wall time: 27.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyutg_K8dz93",
        "colab_type": "code",
        "outputId": "2525d2cf-9722-4dfe-ee31-113ca8f01bce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "# baseline 1 \n",
        "# random labels\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "\n",
        "columns = ['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']\n",
        "\n",
        "model = Pipeline([\n",
        "    ('enc', OneHotEncoder()),\n",
        "    ('est', DummyClassifier(random_state=SEED)),\n",
        "])\n",
        "\n",
        "model.fit(df_train[columns], y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(df_train[columns]), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(df_test[columns]), average='macro'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 0.05887736725599869\n",
            "test 0.060439542712750365\n",
            "CPU times: user 140 ms, sys: 17 ms, total: 157 ms\n",
            "Wall time: 161 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tfRkQdSdz96",
        "colab_type": "code",
        "outputId": "71351344-e4ce-4d4c-e289-732924db001d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "# baseline 2 \n",
        "# pos features + one hot encoding + logistic regression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "columns = ['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']\n",
        "\n",
        "model = Pipeline([\n",
        "    ('enc', OneHotEncoder()),\n",
        "    ('est', LogisticRegressionCV(Cs=5, cv=5, n_jobs=-1, scoring='f1_macro', \n",
        "                             penalty='l2', solver='newton-cg', multi_class='multinomial', random_state=SEED)),\n",
        "])\n",
        "\n",
        "model.fit(df_train[columns], y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(df_train[columns]), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(df_test[columns]), average='macro'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 0.46639500282346874\n",
            "test 0.39660981421559566\n",
            "CPU times: user 4min 47s, sys: 2.85 s, total: 4min 50s\n",
            "Wall time: 24min 14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oScOp2c9dz9-",
        "colab_type": "code",
        "outputId": "f6f58c73-4faa-4e9f-fdf3-3457825e55c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "%%time\n",
        "# baseline 3\n",
        "# use word2vec cbow embedding + baseline 2 + svm\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.svm import LinearSVC\n",
        "import scipy.sparse as sp\n",
        "\n",
        "embeding = w2v_cbow\n",
        "encoder_pos = OneHotEncoder()\n",
        "X_train = sp.hstack([\n",
        "    embeding.transform(df_train.word),\n",
        "    embeding.transform(df_train['next-word']),\n",
        "    embeding.transform(df_train['next-next-word']),\n",
        "    embeding.transform(df_train['prev-word']),\n",
        "    embeding.transform(df_train['prev-prev-word']),\n",
        "    encoder_pos.fit_transform(df_train[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "X_test = sp.hstack([\n",
        "    embeding.transform(df_test.word),\n",
        "    embeding.transform(df_test['next-word']),\n",
        "    embeding.transform(df_test['next-next-word']),\n",
        "    embeding.transform(df_test['prev-word']),\n",
        "    embeding.transform(df_test['prev-prev-word']),\n",
        "    encoder_pos.transform(df_test[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "\n",
        "model = model_selection.GridSearchCV(LinearSVC(penalty='l2', multi_class='ovr', random_state=SEED), \n",
        "                                    {'C': np.logspace(-4, 0, 5)}, \n",
        "                                    cv=3, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(X_train), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(X_test), average='macro'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 14.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train 0.9563954083594406\n",
            "test 0.7888787944895912\n",
            "CPU times: user 3min 14s, sys: 2.09 s, total: 3min 16s\n",
            "Wall time: 17min 35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YfbN16R96km",
        "colab_type": "code",
        "outputId": "59f479c7-2c6b-4e18-fcf7-da8f8f94fdd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/8d/ca8799e7b7cd1c5f84c1e92e6d3dd81ff03957211c959c0d4c027e95888a/catboost-0.20-cp36-none-manylinux1_x86_64.whl (63.6MB)\n",
            "\u001b[K     |████████████████████████████████| 63.6MB 127kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.1.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.17.4)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.25.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.1.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (41.6.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejmhiXUUEc9X",
        "colab_type": "code",
        "outputId": "a55e2015-d3d8-43c1-dce3-15c3d3b3fee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8DU1lvn-_OV",
        "colab_type": "code",
        "outputId": "99ab7a0a-9ba8-41a0-aec5-3ac1768a3bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "# my turn)\n",
        "# use word2vec cbow embedding + baseline 2 + random forest + catboost\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from catboost import CatBoostClassifier\n",
        "import scipy.sparse as sp\n",
        "\n",
        "embeding = w2v_cbow\n",
        "encoder_pos = OneHotEncoder()\n",
        "X_train = sp.hstack([\n",
        "    embeding.transform(df_train.word),\n",
        "    embeding.transform(df_train['next-word']),\n",
        "    embeding.transform(df_train['next-next-word']),\n",
        "    embeding.transform(df_train['prev-word']),\n",
        "    embeding.transform(df_train['prev-prev-word']),\n",
        "    encoder_pos.fit_transform(df_train[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "X_test = sp.hstack([\n",
        "    embeding.transform(df_test.word),\n",
        "    embeding.transform(df_test['next-word']),\n",
        "    embeding.transform(df_test['next-next-word']),\n",
        "    embeding.transform(df_test['prev-word']),\n",
        "    embeding.transform(df_test['prev-prev-word']),\n",
        "    encoder_pos.transform(df_test[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "\n",
        "cat = CatBoostClassifier(random_state=SEED, verbose=True, depth=4, early_stopping_rounds=5, iterations=400)\n",
        "\n",
        "model = model_selection.GridSearchCV(cat, param_grid={\"learning_rate\":(0.5, 0.9)}, scoring='f1_macro', cv=3, n_jobs=-1, verbose=3)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(X_train), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(X_test), average='macro'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed: 107.9min remaining:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed: 107.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.5215154\ttotal: 3.94s\tremaining: 26m 11s\n",
            "1:\tlearn: 2.7394125\ttotal: 7.08s\tremaining: 23m 28s\n",
            "2:\tlearn: 4.7892085\ttotal: 10.4s\tremaining: 22m 51s\n",
            "3:\tlearn: 4.7604891\ttotal: 13.9s\tremaining: 22m 55s\n",
            "4:\tlearn: 4.0366052\ttotal: 17.3s\tremaining: 22m 46s\n",
            "5:\tlearn: 3.2592982\ttotal: 20.6s\tremaining: 22m 30s\n",
            "6:\tlearn: 2.9448930\ttotal: 23.6s\tremaining: 22m 6s\n",
            "7:\tlearn: 3.0799694\ttotal: 26.7s\tremaining: 21m 49s\n",
            "8:\tlearn: 2.4947518\ttotal: 30.1s\tremaining: 21m 46s\n",
            "9:\tlearn: 2.3685982\ttotal: 33.3s\tremaining: 21m 37s\n",
            "10:\tlearn: 2.1922289\ttotal: 36.7s\tremaining: 21m 36s\n",
            "11:\tlearn: 1.9600154\ttotal: 39.7s\tremaining: 21m 24s\n",
            "12:\tlearn: 1.6217023\ttotal: 43s\tremaining: 21m 18s\n",
            "13:\tlearn: 1.2854398\ttotal: 46s\tremaining: 21m 9s\n",
            "14:\tlearn: 1.2632648\ttotal: 49.3s\tremaining: 21m 6s\n",
            "15:\tlearn: 1.2445740\ttotal: 52.5s\tremaining: 21m\n",
            "16:\tlearn: 1.2288518\ttotal: 55.6s\tremaining: 20m 53s\n",
            "17:\tlearn: 1.2115501\ttotal: 58.6s\tremaining: 20m 44s\n",
            "18:\tlearn: 1.1948596\ttotal: 1m 1s\tremaining: 20m 42s\n",
            "19:\tlearn: 1.2127415\ttotal: 1m 5s\tremaining: 20m 45s\n",
            "20:\tlearn: 1.1195148\ttotal: 1m 9s\tremaining: 20m 57s\n",
            "21:\tlearn: 1.0953092\ttotal: 1m 13s\tremaining: 20m 54s\n",
            "22:\tlearn: 1.0721907\ttotal: 1m 16s\tremaining: 20m 47s\n",
            "23:\tlearn: 1.0450936\ttotal: 1m 19s\tremaining: 20m 49s\n",
            "24:\tlearn: 1.0197547\ttotal: 1m 22s\tremaining: 20m 44s\n",
            "25:\tlearn: 0.9946942\ttotal: 1m 26s\tremaining: 20m 37s\n",
            "26:\tlearn: 0.9774027\ttotal: 1m 29s\tremaining: 20m 30s\n",
            "27:\tlearn: 0.9660675\ttotal: 1m 32s\tremaining: 20m 27s\n",
            "28:\tlearn: 0.9414924\ttotal: 1m 35s\tremaining: 20m 22s\n",
            "29:\tlearn: 0.9219239\ttotal: 1m 38s\tremaining: 20m 16s\n",
            "30:\tlearn: 0.9127913\ttotal: 1m 42s\tremaining: 20m 16s\n",
            "31:\tlearn: 0.8868761\ttotal: 1m 45s\tremaining: 20m 11s\n",
            "32:\tlearn: 0.8804330\ttotal: 1m 48s\tremaining: 20m 10s\n",
            "33:\tlearn: 0.8713228\ttotal: 1m 51s\tremaining: 20m 5s\n",
            "34:\tlearn: 0.8697825\ttotal: 1m 55s\tremaining: 20m 1s\n",
            "35:\tlearn: 0.8586090\ttotal: 1m 58s\tremaining: 19m 55s\n",
            "36:\tlearn: 0.8536956\ttotal: 2m 1s\tremaining: 19m 49s\n",
            "37:\tlearn: 0.8514686\ttotal: 2m 4s\tremaining: 19m 46s\n",
            "38:\tlearn: 0.8460646\ttotal: 2m 7s\tremaining: 19m 41s\n",
            "39:\tlearn: 0.8433992\ttotal: 2m 11s\tremaining: 19m 39s\n",
            "40:\tlearn: 0.8354935\ttotal: 2m 14s\tremaining: 19m 39s\n",
            "41:\tlearn: 0.8286830\ttotal: 2m 17s\tremaining: 19m 34s\n",
            "42:\tlearn: 0.8224533\ttotal: 2m 20s\tremaining: 19m 29s\n",
            "43:\tlearn: 0.8176995\ttotal: 2m 24s\tremaining: 19m 32s\n",
            "44:\tlearn: 0.8127561\ttotal: 2m 28s\tremaining: 19m 31s\n",
            "45:\tlearn: 0.8065364\ttotal: 2m 31s\tremaining: 19m 26s\n",
            "46:\tlearn: 0.8026600\ttotal: 2m 34s\tremaining: 19m 21s\n",
            "47:\tlearn: 0.7917064\ttotal: 2m 37s\tremaining: 19m 16s\n",
            "48:\tlearn: 0.7858055\ttotal: 2m 40s\tremaining: 19m 12s\n",
            "49:\tlearn: 0.7807623\ttotal: 2m 44s\tremaining: 19m 12s\n",
            "50:\tlearn: 0.7744913\ttotal: 2m 47s\tremaining: 19m 6s\n",
            "51:\tlearn: 0.7679120\ttotal: 2m 51s\tremaining: 19m 5s\n",
            "52:\tlearn: 0.7633349\ttotal: 2m 54s\tremaining: 19m\n",
            "53:\tlearn: 0.7582991\ttotal: 2m 57s\tremaining: 18m 56s\n",
            "54:\tlearn: 0.7539766\ttotal: 3m\tremaining: 18m 53s\n",
            "55:\tlearn: 0.7500188\ttotal: 3m 3s\tremaining: 18m 49s\n",
            "56:\tlearn: 0.7448170\ttotal: 3m 6s\tremaining: 18m 45s\n",
            "57:\tlearn: 0.7436284\ttotal: 3m 10s\tremaining: 18m 40s\n",
            "58:\tlearn: 0.7319372\ttotal: 3m 13s\tremaining: 18m 40s\n",
            "59:\tlearn: 0.7258673\ttotal: 3m 16s\tremaining: 18m 35s\n",
            "60:\tlearn: 0.7199056\ttotal: 3m 20s\tremaining: 18m 32s\n",
            "61:\tlearn: 0.7137797\ttotal: 3m 23s\tremaining: 18m 28s\n",
            "62:\tlearn: 0.7071387\ttotal: 3m 26s\tremaining: 18m 23s\n",
            "63:\tlearn: 0.7011133\ttotal: 3m 29s\tremaining: 18m 20s\n",
            "64:\tlearn: 0.6962554\ttotal: 3m 32s\tremaining: 18m 16s\n",
            "65:\tlearn: 0.6895076\ttotal: 3m 36s\tremaining: 18m 13s\n",
            "66:\tlearn: 0.6805758\ttotal: 3m 39s\tremaining: 18m 9s\n",
            "67:\tlearn: 0.6739709\ttotal: 3m 42s\tremaining: 18m 5s\n",
            "68:\tlearn: 0.6700567\ttotal: 3m 45s\tremaining: 18m 1s\n",
            "69:\tlearn: 0.6597504\ttotal: 3m 48s\tremaining: 17m 58s\n",
            "70:\tlearn: 0.6535069\ttotal: 3m 51s\tremaining: 17m 54s\n",
            "71:\tlearn: 0.6455713\ttotal: 3m 54s\tremaining: 17m 50s\n",
            "72:\tlearn: 0.6388575\ttotal: 3m 58s\tremaining: 17m 46s\n",
            "73:\tlearn: 0.6316633\ttotal: 4m 1s\tremaining: 17m 42s\n",
            "74:\tlearn: 0.6246095\ttotal: 4m 4s\tremaining: 17m 38s\n",
            "75:\tlearn: 0.6187980\ttotal: 4m 7s\tremaining: 17m 34s\n",
            "76:\tlearn: 0.6128579\ttotal: 4m 10s\tremaining: 17m 30s\n",
            "77:\tlearn: 0.6066439\ttotal: 4m 14s\tremaining: 17m 28s\n",
            "78:\tlearn: 0.6004325\ttotal: 4m 17s\tremaining: 17m 24s\n",
            "79:\tlearn: 0.5962779\ttotal: 4m 20s\tremaining: 17m 21s\n",
            "80:\tlearn: 0.5901177\ttotal: 4m 23s\tremaining: 17m 17s\n",
            "81:\tlearn: 0.5861484\ttotal: 4m 26s\tremaining: 17m 13s\n",
            "82:\tlearn: 0.5818064\ttotal: 4m 29s\tremaining: 17m 9s\n",
            "83:\tlearn: 0.5758873\ttotal: 4m 32s\tremaining: 17m 5s\n",
            "84:\tlearn: 0.5700747\ttotal: 4m 35s\tremaining: 17m 2s\n",
            "85:\tlearn: 0.5634093\ttotal: 4m 39s\tremaining: 16m 58s\n",
            "86:\tlearn: 0.5531110\ttotal: 4m 42s\tremaining: 16m 56s\n",
            "87:\tlearn: 0.5481663\ttotal: 4m 45s\tremaining: 16m 53s\n",
            "88:\tlearn: 0.5472517\ttotal: 4m 49s\tremaining: 16m 51s\n",
            "89:\tlearn: 0.5430580\ttotal: 4m 52s\tremaining: 16m 47s\n",
            "90:\tlearn: 0.5388972\ttotal: 4m 55s\tremaining: 16m 43s\n",
            "91:\tlearn: 0.5362066\ttotal: 4m 59s\tremaining: 16m 41s\n",
            "92:\tlearn: 0.5320061\ttotal: 5m 2s\tremaining: 16m 37s\n",
            "93:\tlearn: 0.5277403\ttotal: 5m 5s\tremaining: 16m 33s\n",
            "94:\tlearn: 0.5237746\ttotal: 5m 8s\tremaining: 16m 30s\n",
            "95:\tlearn: 0.5162460\ttotal: 5m 11s\tremaining: 16m 27s\n",
            "96:\tlearn: 0.5115554\ttotal: 5m 15s\tremaining: 16m 24s\n",
            "97:\tlearn: 0.5095621\ttotal: 5m 18s\tremaining: 16m 20s\n",
            "98:\tlearn: 0.5058223\ttotal: 5m 21s\tremaining: 16m 17s\n",
            "99:\tlearn: 0.5013988\ttotal: 5m 24s\tremaining: 16m 13s\n",
            "100:\tlearn: 0.4997500\ttotal: 5m 27s\tremaining: 16m 10s\n",
            "101:\tlearn: 0.4966802\ttotal: 5m 30s\tremaining: 16m 6s\n",
            "102:\tlearn: 0.4934676\ttotal: 5m 34s\tremaining: 16m 3s\n",
            "103:\tlearn: 0.4897576\ttotal: 5m 37s\tremaining: 15m 59s\n",
            "104:\tlearn: 0.4850771\ttotal: 5m 40s\tremaining: 15m 55s\n",
            "105:\tlearn: 0.4826384\ttotal: 5m 43s\tremaining: 15m 54s\n",
            "106:\tlearn: 0.4798828\ttotal: 5m 47s\tremaining: 15m 50s\n",
            "107:\tlearn: 0.4756328\ttotal: 5m 50s\tremaining: 15m 46s\n",
            "108:\tlearn: 0.4728715\ttotal: 5m 53s\tremaining: 15m 42s\n",
            "109:\tlearn: 0.4698181\ttotal: 5m 56s\tremaining: 15m 39s\n",
            "110:\tlearn: 0.4668699\ttotal: 5m 59s\tremaining: 15m 35s\n",
            "111:\tlearn: 0.4618942\ttotal: 6m 2s\tremaining: 15m 31s\n",
            "112:\tlearn: 0.4596428\ttotal: 6m 5s\tremaining: 15m 27s\n",
            "113:\tlearn: 0.4566322\ttotal: 6m 8s\tremaining: 15m 24s\n",
            "114:\tlearn: 0.4539053\ttotal: 6m 11s\tremaining: 15m 20s\n",
            "115:\tlearn: 0.4508221\ttotal: 6m 14s\tremaining: 15m 16s\n",
            "116:\tlearn: 0.4479455\ttotal: 6m 17s\tremaining: 15m 12s\n",
            "117:\tlearn: 0.4454696\ttotal: 6m 20s\tremaining: 15m 9s\n",
            "118:\tlearn: 0.4431584\ttotal: 6m 23s\tremaining: 15m 5s\n",
            "119:\tlearn: 0.4402640\ttotal: 6m 26s\tremaining: 15m 2s\n",
            "120:\tlearn: 0.4358197\ttotal: 6m 29s\tremaining: 14m 58s\n",
            "121:\tlearn: 0.4316767\ttotal: 6m 32s\tremaining: 14m 55s\n",
            "122:\tlearn: 0.4288749\ttotal: 6m 35s\tremaining: 14m 51s\n",
            "123:\tlearn: 0.4263377\ttotal: 6m 39s\tremaining: 14m 48s\n",
            "124:\tlearn: 0.4237859\ttotal: 6m 42s\tremaining: 14m 44s\n",
            "125:\tlearn: 0.4213414\ttotal: 6m 45s\tremaining: 14m 41s\n",
            "126:\tlearn: 0.4187624\ttotal: 6m 48s\tremaining: 14m 37s\n",
            "127:\tlearn: 0.4163434\ttotal: 6m 51s\tremaining: 14m 34s\n",
            "128:\tlearn: 0.4139301\ttotal: 6m 54s\tremaining: 14m 30s\n",
            "129:\tlearn: 0.4110842\ttotal: 6m 57s\tremaining: 14m 27s\n",
            "130:\tlearn: 0.4082559\ttotal: 7m\tremaining: 14m 23s\n",
            "131:\tlearn: 0.4054344\ttotal: 7m 3s\tremaining: 14m 20s\n",
            "132:\tlearn: 0.4026914\ttotal: 7m 6s\tremaining: 14m 16s\n",
            "133:\tlearn: 0.3998695\ttotal: 7m 9s\tremaining: 14m 13s\n",
            "134:\tlearn: 0.3971514\ttotal: 7m 12s\tremaining: 14m 9s\n",
            "135:\tlearn: 0.3947305\ttotal: 7m 15s\tremaining: 14m 6s\n",
            "136:\tlearn: 0.3942890\ttotal: 7m 18s\tremaining: 14m 2s\n",
            "137:\tlearn: 0.3934190\ttotal: 7m 22s\tremaining: 13m 59s\n",
            "138:\tlearn: 0.3921905\ttotal: 7m 25s\tremaining: 13m 55s\n",
            "139:\tlearn: 0.3892803\ttotal: 7m 28s\tremaining: 13m 53s\n",
            "140:\tlearn: 0.3885216\ttotal: 7m 31s\tremaining: 13m 49s\n",
            "141:\tlearn: 0.3876636\ttotal: 7m 34s\tremaining: 13m 46s\n",
            "142:\tlearn: 0.3870319\ttotal: 7m 37s\tremaining: 13m 42s\n",
            "143:\tlearn: 0.3857407\ttotal: 7m 41s\tremaining: 13m 39s\n",
            "144:\tlearn: 0.3849606\ttotal: 7m 44s\tremaining: 13m 36s\n",
            "145:\tlearn: 0.3835754\ttotal: 7m 47s\tremaining: 13m 33s\n",
            "146:\tlearn: 0.3822682\ttotal: 7m 50s\tremaining: 13m 30s\n",
            "147:\tlearn: 0.3817002\ttotal: 7m 53s\tremaining: 13m 26s\n",
            "148:\tlearn: 0.3809109\ttotal: 7m 56s\tremaining: 13m 23s\n",
            "149:\tlearn: 0.3793566\ttotal: 8m\tremaining: 13m 20s\n",
            "150:\tlearn: 0.3786600\ttotal: 8m 3s\tremaining: 13m 17s\n",
            "151:\tlearn: 0.3771978\ttotal: 8m 6s\tremaining: 13m 14s\n",
            "152:\tlearn: 0.3761347\ttotal: 8m 10s\tremaining: 13m 12s\n",
            "153:\tlearn: 0.3755957\ttotal: 8m 13s\tremaining: 13m 8s\n",
            "154:\tlearn: 0.3747171\ttotal: 8m 17s\tremaining: 13m 6s\n",
            "155:\tlearn: 0.3741901\ttotal: 8m 20s\tremaining: 13m 3s\n",
            "156:\tlearn: 0.3736439\ttotal: 8m 23s\tremaining: 12m 59s\n",
            "157:\tlearn: 0.3728423\ttotal: 8m 26s\tremaining: 12m 56s\n",
            "158:\tlearn: 0.3719855\ttotal: 8m 30s\tremaining: 12m 53s\n",
            "159:\tlearn: 0.3711310\ttotal: 8m 33s\tremaining: 12m 50s\n",
            "160:\tlearn: 0.3701980\ttotal: 8m 36s\tremaining: 12m 47s\n",
            "161:\tlearn: 0.3696095\ttotal: 8m 39s\tremaining: 12m 43s\n",
            "162:\tlearn: 0.3690793\ttotal: 8m 42s\tremaining: 12m 40s\n",
            "163:\tlearn: 0.3681449\ttotal: 8m 46s\tremaining: 12m 37s\n",
            "164:\tlearn: 0.3673343\ttotal: 8m 49s\tremaining: 12m 33s\n",
            "165:\tlearn: 0.3668273\ttotal: 8m 52s\tremaining: 12m 30s\n",
            "166:\tlearn: 0.3664226\ttotal: 8m 55s\tremaining: 12m 27s\n",
            "167:\tlearn: 0.3653556\ttotal: 8m 58s\tremaining: 12m 24s\n",
            "168:\tlearn: 0.3648832\ttotal: 9m 1s\tremaining: 12m 20s\n",
            "169:\tlearn: 0.3639902\ttotal: 9m 5s\tremaining: 12m 17s\n",
            "170:\tlearn: 0.3633858\ttotal: 9m 8s\tremaining: 12m 14s\n",
            "171:\tlearn: 0.3626035\ttotal: 9m 11s\tremaining: 12m 10s\n",
            "172:\tlearn: 0.3619032\ttotal: 9m 14s\tremaining: 12m 7s\n",
            "173:\tlearn: 0.3610996\ttotal: 9m 17s\tremaining: 12m 4s\n",
            "174:\tlearn: 0.3603098\ttotal: 9m 20s\tremaining: 12m 1s\n",
            "175:\tlearn: 0.3600466\ttotal: 9m 24s\tremaining: 11m 57s\n",
            "176:\tlearn: 0.3593876\ttotal: 9m 27s\tremaining: 11m 54s\n",
            "177:\tlearn: 0.3583439\ttotal: 9m 30s\tremaining: 11m 51s\n",
            "178:\tlearn: 0.3579573\ttotal: 9m 33s\tremaining: 11m 48s\n",
            "179:\tlearn: 0.3564572\ttotal: 9m 36s\tremaining: 11m 44s\n",
            "180:\tlearn: 0.3559066\ttotal: 9m 39s\tremaining: 11m 41s\n",
            "181:\tlearn: 0.3534659\ttotal: 9m 43s\tremaining: 11m 38s\n",
            "182:\tlearn: 0.3526752\ttotal: 9m 46s\tremaining: 11m 35s\n",
            "183:\tlearn: 0.3520759\ttotal: 9m 50s\tremaining: 11m 32s\n",
            "184:\tlearn: 0.3516400\ttotal: 9m 53s\tremaining: 11m 29s\n",
            "185:\tlearn: 0.3512689\ttotal: 9m 56s\tremaining: 11m 25s\n",
            "186:\tlearn: 0.3503490\ttotal: 9m 59s\tremaining: 11m 22s\n",
            "187:\tlearn: 0.3501047\ttotal: 10m 2s\tremaining: 11m 19s\n",
            "188:\tlearn: 0.3494867\ttotal: 10m 5s\tremaining: 11m 16s\n",
            "189:\tlearn: 0.3491417\ttotal: 10m 9s\tremaining: 11m 13s\n",
            "190:\tlearn: 0.3487534\ttotal: 10m 12s\tremaining: 11m 9s\n",
            "191:\tlearn: 0.3480751\ttotal: 10m 15s\tremaining: 11m 6s\n",
            "192:\tlearn: 0.3474927\ttotal: 10m 19s\tremaining: 11m 4s\n",
            "193:\tlearn: 0.3465895\ttotal: 10m 22s\tremaining: 11m\n",
            "194:\tlearn: 0.3453563\ttotal: 10m 25s\tremaining: 10m 57s\n",
            "195:\tlearn: 0.3448214\ttotal: 10m 28s\tremaining: 10m 54s\n",
            "196:\tlearn: 0.3439888\ttotal: 10m 32s\tremaining: 10m 51s\n",
            "197:\tlearn: 0.3433347\ttotal: 10m 35s\tremaining: 10m 48s\n",
            "198:\tlearn: 0.3423398\ttotal: 10m 39s\tremaining: 10m 45s\n",
            "199:\tlearn: 0.3418075\ttotal: 10m 42s\tremaining: 10m 42s\n",
            "200:\tlearn: 0.3413483\ttotal: 10m 45s\tremaining: 10m 39s\n",
            "201:\tlearn: 0.3402239\ttotal: 10m 48s\tremaining: 10m 36s\n",
            "202:\tlearn: 0.3384977\ttotal: 10m 52s\tremaining: 10m 33s\n",
            "203:\tlearn: 0.3380085\ttotal: 10m 55s\tremaining: 10m 30s\n",
            "204:\tlearn: 0.3373907\ttotal: 10m 58s\tremaining: 10m 26s\n",
            "205:\tlearn: 0.3364324\ttotal: 11m 1s\tremaining: 10m 23s\n",
            "206:\tlearn: 0.3359782\ttotal: 11m 5s\tremaining: 10m 20s\n",
            "207:\tlearn: 0.3349739\ttotal: 11m 8s\tremaining: 10m 16s\n",
            "208:\tlearn: 0.3335873\ttotal: 11m 11s\tremaining: 10m 13s\n",
            "209:\tlearn: 0.3331255\ttotal: 11m 14s\tremaining: 10m 10s\n",
            "210:\tlearn: 0.3323903\ttotal: 11m 17s\tremaining: 10m 7s\n",
            "211:\tlearn: 0.3318521\ttotal: 11m 20s\tremaining: 10m 3s\n",
            "212:\tlearn: 0.3309584\ttotal: 11m 23s\tremaining: 10m\n",
            "213:\tlearn: 0.3303442\ttotal: 11m 27s\tremaining: 9m 57s\n",
            "214:\tlearn: 0.3297387\ttotal: 11m 30s\tremaining: 9m 54s\n",
            "215:\tlearn: 0.3290160\ttotal: 11m 33s\tremaining: 9m 50s\n",
            "216:\tlearn: 0.3286737\ttotal: 11m 36s\tremaining: 9m 47s\n",
            "217:\tlearn: 0.3282646\ttotal: 11m 40s\tremaining: 9m 44s\n",
            "218:\tlearn: 0.3269905\ttotal: 11m 43s\tremaining: 9m 41s\n",
            "219:\tlearn: 0.3261198\ttotal: 11m 47s\tremaining: 9m 38s\n",
            "220:\tlearn: 0.3256976\ttotal: 11m 50s\tremaining: 9m 35s\n",
            "221:\tlearn: 0.3253813\ttotal: 11m 53s\tremaining: 9m 32s\n",
            "222:\tlearn: 0.3251413\ttotal: 11m 56s\tremaining: 9m 28s\n",
            "223:\tlearn: 0.3249973\ttotal: 11m 59s\tremaining: 9m 25s\n",
            "224:\tlearn: 0.3244904\ttotal: 12m 2s\tremaining: 9m 21s\n",
            "225:\tlearn: 0.3232304\ttotal: 12m 5s\tremaining: 9m 18s\n",
            "226:\tlearn: 0.3227606\ttotal: 12m 8s\tremaining: 9m 15s\n",
            "227:\tlearn: 0.3223199\ttotal: 12m 11s\tremaining: 9m 12s\n",
            "228:\tlearn: 0.3220418\ttotal: 12m 14s\tremaining: 9m 8s\n",
            "229:\tlearn: 0.3214997\ttotal: 12m 17s\tremaining: 9m 5s\n",
            "230:\tlearn: 0.3210709\ttotal: 12m 20s\tremaining: 9m 2s\n",
            "231:\tlearn: 0.3205755\ttotal: 12m 24s\tremaining: 8m 58s\n",
            "232:\tlearn: 0.3201083\ttotal: 12m 27s\tremaining: 8m 55s\n",
            "233:\tlearn: 0.3189927\ttotal: 12m 30s\tremaining: 8m 52s\n",
            "234:\tlearn: 0.3183354\ttotal: 12m 34s\tremaining: 8m 49s\n",
            "235:\tlearn: 0.3179741\ttotal: 12m 37s\tremaining: 8m 46s\n",
            "236:\tlearn: 0.3177956\ttotal: 12m 40s\tremaining: 8m 42s\n",
            "237:\tlearn: 0.3171782\ttotal: 12m 43s\tremaining: 8m 39s\n",
            "238:\tlearn: 0.3166072\ttotal: 12m 46s\tremaining: 8m 36s\n",
            "239:\tlearn: 0.3159441\ttotal: 12m 50s\tremaining: 8m 33s\n",
            "240:\tlearn: 0.3155004\ttotal: 12m 53s\tremaining: 8m 30s\n",
            "241:\tlearn: 0.3148483\ttotal: 12m 57s\tremaining: 8m 27s\n",
            "242:\tlearn: 0.3146070\ttotal: 13m\tremaining: 8m 24s\n",
            "243:\tlearn: 0.3142767\ttotal: 13m 3s\tremaining: 8m 21s\n",
            "244:\tlearn: 0.3138483\ttotal: 13m 7s\tremaining: 8m 17s\n",
            "245:\tlearn: 0.3134927\ttotal: 13m 10s\tremaining: 8m 14s\n",
            "246:\tlearn: 0.3132555\ttotal: 13m 13s\tremaining: 8m 11s\n",
            "247:\tlearn: 0.3131494\ttotal: 13m 16s\tremaining: 8m 7s\n",
            "248:\tlearn: 0.3120608\ttotal: 13m 19s\tremaining: 8m 4s\n",
            "249:\tlearn: 0.3117303\ttotal: 13m 22s\tremaining: 8m 1s\n",
            "250:\tlearn: 0.3114973\ttotal: 13m 25s\tremaining: 7m 58s\n",
            "251:\tlearn: 0.3112818\ttotal: 13m 28s\tremaining: 7m 54s\n",
            "252:\tlearn: 0.3106818\ttotal: 13m 32s\tremaining: 7m 51s\n",
            "253:\tlearn: 0.3096765\ttotal: 13m 35s\tremaining: 7m 48s\n",
            "254:\tlearn: 0.3093431\ttotal: 13m 38s\tremaining: 7m 45s\n",
            "255:\tlearn: 0.3090781\ttotal: 13m 41s\tremaining: 7m 42s\n",
            "256:\tlearn: 0.3087170\ttotal: 13m 44s\tremaining: 7m 38s\n",
            "257:\tlearn: 0.3084617\ttotal: 13m 47s\tremaining: 7m 35s\n",
            "258:\tlearn: 0.3077321\ttotal: 13m 50s\tremaining: 7m 32s\n",
            "259:\tlearn: 0.3074798\ttotal: 13m 53s\tremaining: 7m 29s\n",
            "260:\tlearn: 0.3071690\ttotal: 13m 57s\tremaining: 7m 25s\n",
            "261:\tlearn: 0.3067044\ttotal: 14m\tremaining: 7m 22s\n",
            "262:\tlearn: 0.3062171\ttotal: 14m 3s\tremaining: 7m 19s\n",
            "263:\tlearn: 0.3059979\ttotal: 14m 6s\tremaining: 7m 16s\n",
            "264:\tlearn: 0.3055884\ttotal: 14m 9s\tremaining: 7m 12s\n",
            "265:\tlearn: 0.3046930\ttotal: 14m 12s\tremaining: 7m 9s\n",
            "266:\tlearn: 0.3039752\ttotal: 14m 15s\tremaining: 7m 6s\n",
            "267:\tlearn: 0.3031670\ttotal: 14m 19s\tremaining: 7m 3s\n",
            "268:\tlearn: 0.3029768\ttotal: 14m 22s\tremaining: 6m 59s\n",
            "269:\tlearn: 0.3027874\ttotal: 14m 25s\tremaining: 6m 56s\n",
            "270:\tlearn: 0.3024089\ttotal: 14m 28s\tremaining: 6m 53s\n",
            "271:\tlearn: 0.3011015\ttotal: 14m 32s\tremaining: 6m 50s\n",
            "272:\tlearn: 0.3000611\ttotal: 14m 35s\tremaining: 6m 47s\n",
            "273:\tlearn: 0.2989159\ttotal: 14m 39s\tremaining: 6m 44s\n",
            "274:\tlearn: 0.2981432\ttotal: 14m 42s\tremaining: 6m 41s\n",
            "275:\tlearn: 0.2979027\ttotal: 14m 45s\tremaining: 6m 37s\n",
            "276:\tlearn: 0.2976254\ttotal: 14m 48s\tremaining: 6m 34s\n",
            "277:\tlearn: 0.2967268\ttotal: 14m 51s\tremaining: 6m 31s\n",
            "278:\tlearn: 0.2964546\ttotal: 14m 54s\tremaining: 6m 28s\n",
            "279:\tlearn: 0.2960603\ttotal: 14m 57s\tremaining: 6m 24s\n",
            "280:\tlearn: 0.2958047\ttotal: 15m\tremaining: 6m 21s\n",
            "281:\tlearn: 0.2953852\ttotal: 15m 4s\tremaining: 6m 18s\n",
            "282:\tlearn: 0.2949942\ttotal: 15m 7s\tremaining: 6m 15s\n",
            "283:\tlearn: 0.2940967\ttotal: 15m 10s\tremaining: 6m 11s\n",
            "284:\tlearn: 0.2938513\ttotal: 15m 13s\tremaining: 6m 8s\n",
            "285:\tlearn: 0.2935895\ttotal: 15m 16s\tremaining: 6m 5s\n",
            "286:\tlearn: 0.2925310\ttotal: 15m 20s\tremaining: 6m 2s\n",
            "287:\tlearn: 0.2921671\ttotal: 15m 23s\tremaining: 5m 59s\n",
            "288:\tlearn: 0.2915255\ttotal: 15m 26s\tremaining: 5m 55s\n",
            "289:\tlearn: 0.2910920\ttotal: 15m 29s\tremaining: 5m 52s\n",
            "290:\tlearn: 0.2908366\ttotal: 15m 32s\tremaining: 5m 49s\n",
            "291:\tlearn: 0.2898925\ttotal: 15m 35s\tremaining: 5m 46s\n",
            "292:\tlearn: 0.2888625\ttotal: 15m 38s\tremaining: 5m 42s\n",
            "293:\tlearn: 0.2879356\ttotal: 15m 42s\tremaining: 5m 39s\n",
            "294:\tlearn: 0.2875711\ttotal: 15m 45s\tremaining: 5m 36s\n",
            "295:\tlearn: 0.2873059\ttotal: 15m 48s\tremaining: 5m 33s\n",
            "296:\tlearn: 0.2871513\ttotal: 15m 51s\tremaining: 5m 30s\n",
            "297:\tlearn: 0.2865699\ttotal: 15m 54s\tremaining: 5m 26s\n",
            "298:\tlearn: 0.2862505\ttotal: 15m 57s\tremaining: 5m 23s\n",
            "299:\tlearn: 0.2853664\ttotal: 16m\tremaining: 5m 20s\n",
            "300:\tlearn: 0.2850482\ttotal: 16m 3s\tremaining: 5m 17s\n",
            "301:\tlearn: 0.2846017\ttotal: 16m 7s\tremaining: 5m 13s\n",
            "302:\tlearn: 0.2842749\ttotal: 16m 10s\tremaining: 5m 10s\n",
            "303:\tlearn: 0.2840830\ttotal: 16m 13s\tremaining: 5m 7s\n",
            "304:\tlearn: 0.2839623\ttotal: 16m 16s\tremaining: 5m 4s\n",
            "305:\tlearn: 0.2837386\ttotal: 16m 19s\tremaining: 5m\n",
            "306:\tlearn: 0.2832252\ttotal: 16m 22s\tremaining: 4m 57s\n",
            "307:\tlearn: 0.2822873\ttotal: 16m 26s\tremaining: 4m 54s\n",
            "308:\tlearn: 0.2820544\ttotal: 16m 29s\tremaining: 4m 51s\n",
            "309:\tlearn: 0.2810541\ttotal: 16m 32s\tremaining: 4m 48s\n",
            "310:\tlearn: 0.2800149\ttotal: 16m 36s\tremaining: 4m 45s\n",
            "311:\tlearn: 0.2798182\ttotal: 16m 39s\tremaining: 4m 41s\n",
            "312:\tlearn: 0.2787238\ttotal: 16m 42s\tremaining: 4m 38s\n",
            "313:\tlearn: 0.2785597\ttotal: 16m 45s\tremaining: 4m 35s\n",
            "314:\tlearn: 0.2783506\ttotal: 16m 48s\tremaining: 4m 32s\n",
            "315:\tlearn: 0.2772626\ttotal: 16m 52s\tremaining: 4m 29s\n",
            "316:\tlearn: 0.2770378\ttotal: 16m 55s\tremaining: 4m 25s\n",
            "317:\tlearn: 0.2767375\ttotal: 16m 58s\tremaining: 4m 22s\n",
            "318:\tlearn: 0.2765288\ttotal: 17m 1s\tremaining: 4m 19s\n",
            "319:\tlearn: 0.2762747\ttotal: 17m 4s\tremaining: 4m 16s\n",
            "320:\tlearn: 0.2753950\ttotal: 17m 7s\tremaining: 4m 12s\n",
            "321:\tlearn: 0.2745792\ttotal: 17m 11s\tremaining: 4m 9s\n",
            "322:\tlearn: 0.2741525\ttotal: 17m 14s\tremaining: 4m 6s\n",
            "323:\tlearn: 0.2737670\ttotal: 17m 17s\tremaining: 4m 3s\n",
            "324:\tlearn: 0.2729489\ttotal: 17m 20s\tremaining: 4m\n",
            "325:\tlearn: 0.2725845\ttotal: 17m 23s\tremaining: 3m 56s\n",
            "326:\tlearn: 0.2717302\ttotal: 17m 26s\tremaining: 3m 53s\n",
            "327:\tlearn: 0.2713265\ttotal: 17m 29s\tremaining: 3m 50s\n",
            "328:\tlearn: 0.2709217\ttotal: 17m 32s\tremaining: 3m 47s\n",
            "329:\tlearn: 0.2707079\ttotal: 17m 35s\tremaining: 3m 43s\n",
            "330:\tlearn: 0.2699371\ttotal: 17m 38s\tremaining: 3m 40s\n",
            "331:\tlearn: 0.2691359\ttotal: 17m 42s\tremaining: 3m 37s\n",
            "332:\tlearn: 0.2683164\ttotal: 17m 44s\tremaining: 3m 34s\n",
            "333:\tlearn: 0.2676069\ttotal: 17m 47s\tremaining: 3m 31s\n",
            "334:\tlearn: 0.2674249\ttotal: 17m 51s\tremaining: 3m 27s\n",
            "335:\tlearn: 0.2663190\ttotal: 17m 54s\tremaining: 3m 24s\n",
            "336:\tlearn: 0.2651188\ttotal: 17m 57s\tremaining: 3m 21s\n",
            "337:\tlearn: 0.2648426\ttotal: 18m\tremaining: 3m 18s\n",
            "338:\tlearn: 0.2645906\ttotal: 18m 3s\tremaining: 3m 14s\n",
            "339:\tlearn: 0.2644988\ttotal: 18m 6s\tremaining: 3m 11s\n",
            "340:\tlearn: 0.2639468\ttotal: 18m 9s\tremaining: 3m 8s\n",
            "341:\tlearn: 0.2636132\ttotal: 18m 12s\tremaining: 3m 5s\n",
            "342:\tlearn: 0.2624605\ttotal: 18m 16s\tremaining: 3m 2s\n",
            "343:\tlearn: 0.2622653\ttotal: 18m 19s\tremaining: 2m 58s\n",
            "344:\tlearn: 0.2614415\ttotal: 18m 22s\tremaining: 2m 55s\n",
            "345:\tlearn: 0.2604648\ttotal: 18m 26s\tremaining: 2m 52s\n",
            "346:\tlearn: 0.2601816\ttotal: 18m 29s\tremaining: 2m 49s\n",
            "347:\tlearn: 0.2592356\ttotal: 18m 32s\tremaining: 2m 46s\n",
            "348:\tlearn: 0.2582404\ttotal: 18m 35s\tremaining: 2m 43s\n",
            "349:\tlearn: 0.2576295\ttotal: 18m 38s\tremaining: 2m 39s\n",
            "350:\tlearn: 0.2574272\ttotal: 18m 41s\tremaining: 2m 36s\n",
            "351:\tlearn: 0.2573268\ttotal: 18m 44s\tremaining: 2m 33s\n",
            "352:\tlearn: 0.2570049\ttotal: 18m 48s\tremaining: 2m 30s\n",
            "353:\tlearn: 0.2566267\ttotal: 18m 51s\tremaining: 2m 26s\n",
            "354:\tlearn: 0.2560477\ttotal: 18m 54s\tremaining: 2m 23s\n",
            "355:\tlearn: 0.2550990\ttotal: 18m 57s\tremaining: 2m 20s\n",
            "356:\tlearn: 0.2547357\ttotal: 19m 1s\tremaining: 2m 17s\n",
            "357:\tlearn: 0.2539008\ttotal: 19m 4s\tremaining: 2m 14s\n",
            "358:\tlearn: 0.2535377\ttotal: 19m 7s\tremaining: 2m 11s\n",
            "359:\tlearn: 0.2533898\ttotal: 19m 10s\tremaining: 2m 7s\n",
            "360:\tlearn: 0.2525850\ttotal: 19m 13s\tremaining: 2m 4s\n",
            "361:\tlearn: 0.2516607\ttotal: 19m 16s\tremaining: 2m 1s\n",
            "362:\tlearn: 0.2505291\ttotal: 19m 20s\tremaining: 1m 58s\n",
            "363:\tlearn: 0.2498156\ttotal: 19m 23s\tremaining: 1m 55s\n",
            "364:\tlearn: 0.2488282\ttotal: 19m 27s\tremaining: 1m 51s\n",
            "365:\tlearn: 0.2480643\ttotal: 19m 30s\tremaining: 1m 48s\n",
            "366:\tlearn: 0.2472585\ttotal: 19m 33s\tremaining: 1m 45s\n",
            "367:\tlearn: 0.2469479\ttotal: 19m 36s\tremaining: 1m 42s\n",
            "368:\tlearn: 0.2459620\ttotal: 19m 39s\tremaining: 1m 39s\n",
            "369:\tlearn: 0.2457393\ttotal: 19m 42s\tremaining: 1m 35s\n",
            "370:\tlearn: 0.2455758\ttotal: 19m 45s\tremaining: 1m 32s\n",
            "371:\tlearn: 0.2446290\ttotal: 19m 49s\tremaining: 1m 29s\n",
            "372:\tlearn: 0.2437053\ttotal: 19m 53s\tremaining: 1m 26s\n",
            "373:\tlearn: 0.2432825\ttotal: 19m 56s\tremaining: 1m 23s\n",
            "374:\tlearn: 0.2427855\ttotal: 19m 59s\tremaining: 1m 19s\n",
            "375:\tlearn: 0.2425950\ttotal: 20m 2s\tremaining: 1m 16s\n",
            "376:\tlearn: 0.2416030\ttotal: 20m 6s\tremaining: 1m 13s\n",
            "377:\tlearn: 0.2413153\ttotal: 20m 9s\tremaining: 1m 10s\n",
            "378:\tlearn: 0.2410353\ttotal: 20m 12s\tremaining: 1m 7s\n",
            "379:\tlearn: 0.2409175\ttotal: 20m 15s\tremaining: 1m 3s\n",
            "380:\tlearn: 0.2406361\ttotal: 20m 18s\tremaining: 1m\n",
            "381:\tlearn: 0.2404098\ttotal: 20m 21s\tremaining: 57.6s\n",
            "382:\tlearn: 0.2394206\ttotal: 20m 24s\tremaining: 54.4s\n",
            "383:\tlearn: 0.2380708\ttotal: 20m 28s\tremaining: 51.2s\n",
            "384:\tlearn: 0.2378138\ttotal: 20m 31s\tremaining: 48s\n",
            "385:\tlearn: 0.2368906\ttotal: 20m 34s\tremaining: 44.8s\n",
            "386:\tlearn: 0.2365997\ttotal: 20m 37s\tremaining: 41.6s\n",
            "387:\tlearn: 0.2356924\ttotal: 20m 41s\tremaining: 38.4s\n",
            "388:\tlearn: 0.2349159\ttotal: 20m 44s\tremaining: 35.2s\n",
            "389:\tlearn: 0.2338738\ttotal: 20m 47s\tremaining: 32s\n",
            "390:\tlearn: 0.2328317\ttotal: 20m 50s\tremaining: 28.8s\n",
            "391:\tlearn: 0.2320187\ttotal: 20m 53s\tremaining: 25.6s\n",
            "392:\tlearn: 0.2314210\ttotal: 20m 56s\tremaining: 22.4s\n",
            "393:\tlearn: 0.2312806\ttotal: 21m\tremaining: 19.2s\n",
            "394:\tlearn: 0.2311107\ttotal: 21m 3s\tremaining: 16s\n",
            "395:\tlearn: 0.2308411\ttotal: 21m 6s\tremaining: 12.8s\n",
            "396:\tlearn: 0.2300650\ttotal: 21m 9s\tremaining: 9.59s\n",
            "397:\tlearn: 0.2297907\ttotal: 21m 12s\tremaining: 6.39s\n",
            "398:\tlearn: 0.2296608\ttotal: 21m 15s\tremaining: 3.2s\n",
            "399:\tlearn: 0.2289284\ttotal: 21m 18s\tremaining: 0us\n",
            "train 0.8360099260282032\n",
            "test 0.61543579957225\n",
            "CPU times: user 42min 50s, sys: 13.3 s, total: 43min 4s\n",
            "Wall time: 2h 10min 19s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxPREg1m_DA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}