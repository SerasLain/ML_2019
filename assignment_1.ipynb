{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrl74kV57-Hf",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
        "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
        "2. Tokenize text by BPE with vocab_size = 100\n",
        "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
        "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
        "5. Calculate perplexity of the language model for the first sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arA4XeuV9HyH",
        "colab_type": "code",
        "outputId": "1f6d8538-1ea1-4e0e-80d7-958a0c509750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget \"http://www.gutenberg.org/files/2600/2600-0.txt\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-06 16:50:51--  http://www.gutenberg.org/files/2600/2600-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3359549 (3.2M) [text/plain]\n",
            "Saving to: ‘2600-0.txt.1’\n",
            "\n",
            "2600-0.txt.1        100%[===================>]   3.20M  4.58MB/s    in 0.7s    \n",
            "\n",
            "2019-10-06 16:50:55 (4.58 MB/s) - ‘2600-0.txt.1’ saved [3359549/3359549]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEpldTuf7-Hi",
        "colab_type": "code",
        "outputId": "d5fd55ce-a55c-424d-ad22-f4955fbc5cf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = open('2600-0.txt', 'r', encoding='utf-8').read()[2:]\n",
        "len(text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3227579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSGLbE_q7-Hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    punct = '!\"#$%&\\'()*+,-—/:;<=>?@[\\\\]^_`{|}~«»”’“‘'\n",
        "    regex = re.compile('[%s]' % re.escape(punct))\n",
        "    t = text.lower()    \n",
        "    t = regex.sub(' ', t)\n",
        "    t = re.sub(r'\\s+', r' ', t)\n",
        "    return t\n",
        "\n",
        "text = preprocess_text(text)\n",
        "assert len(text) == 3141169"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVDxSiT77-Hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = text.split('.')\n",
        "text = [x.strip() for x in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoXDoUoE7-H0",
        "colab_type": "code",
        "outputId": "9769441b-b88e-4d69-a95d-96f1876a050c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(set(text)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHREcGY4UMHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def code_text(text, stoi):\n",
        "    \"\"\"\n",
        "    Encodes symbols with ids\n",
        "    :param text: list if lists with strings\n",
        "    :param stoi: dict, character:character_id\n",
        "    :return: encoded text, list of lists of integers\n",
        "    \"\"\"\n",
        "    \n",
        "    coded_text = []\n",
        "    for sent in text:\n",
        "        coded_sent = []\n",
        "        for char in sent:\n",
        "            try:\n",
        "                coded_sent.append(stoi[char])\n",
        "            except KeyError:\n",
        "                print(char)\n",
        "        coded_text.append(coded_sent)\n",
        "    return coded_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNbXwJKSHbJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_token(token, _id, text):\n",
        "    \"\"\"\n",
        "    Replace token with new id in text\n",
        "    :param token: tuple\n",
        "    :param _id: int, new id\n",
        "    :param text: list of list of ids\n",
        "    :return: list of list of ids, modified text\n",
        "    \"\"\"\n",
        "    \n",
        "    text_new = [sent[:] for sent in text]\n",
        "    for i, sent in enumerate(text):\n",
        "        modified = 0\n",
        "        for j, (v, w) in enumerate(zip(sent[:-1], sent[1:])):\n",
        "            if (v, w) == token:\n",
        "                text_new[i][j - modified] = _id\n",
        "                del text_new[i][j - modified + 1]\n",
        "                modified += 1\n",
        "    return text_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6v64rw8Qqzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def count_ngrams(text, n):\n",
        "    \"\"\"\n",
        "    Counts frequency of ngrams\n",
        "\n",
        "    :param text: list of lists\n",
        "\n",
        "    :return: Counter of ngrams in text\n",
        "    \"\"\"\n",
        "    bigrams = []\n",
        "    for sent in text:\n",
        "        for i in range(len(sent) - (n - 1)):\n",
        "            bigrams.append(tuple((sent[i: i + n])))\n",
        "\n",
        "    counted_bigr = Counter(bigrams)\n",
        "\n",
        "    return counted_bigr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZC6IGaor7-H3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import TransformerMixin\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class BPE(TransformerMixin):\n",
        "    def __init__(self, vocab_size=100):\n",
        "        super(BPE, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # index to token\n",
        "        self.itos = []\n",
        "        # token to index\n",
        "        self.stoi = {}\n",
        "\n",
        "        \n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        fit itos and stoi\n",
        "        text: list of strings \n",
        "        \"\"\"    \n",
        "\n",
        "        # tokenize text by symbols and fill in self.itos and self.stoi\n",
        "        self.itos = list(set(''.join(text)))\n",
        "        \n",
        "        self.stoi = {}\n",
        "        for i in range(len(self.itos)):\n",
        "            self.stoi[self.itos[i]] = i\n",
        "\n",
        "        bigr = count_ngrams(text, 2)\n",
        "        text = code_text(text, self.stoi)\n",
        "\n",
        "        while len(self.itos) < self.vocab_size:\n",
        "            bigrams = count_ngrams(text, 2)\n",
        "            most_comm = bigrams.most_common(1)\n",
        "            if most_comm != []:\n",
        "                new_token = bigrams.most_common(1)[0][0] # most common bigram\n",
        "                new_id = len(self.itos)\n",
        "                self.itos.append(new_token)\n",
        "                self.stoi[new_token] = new_id\n",
        "                text = replace_token(new_token, new_id, text)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def transform(self, text):\n",
        "        \"\"\"\n",
        "        convert text to a sequence of token ids\n",
        "        text: list of strings\n",
        "        \"\"\"\n",
        "        # tokenize text by symbols using self.stoi\n",
        "        text = code_text(text, self.stoi)\n",
        " \n",
        "\n",
        "        for _id, token in enumerate(self.itos):\n",
        "            text = replace_token(token, _id, text)\n",
        "\n",
        "        return text\n",
        "    \n",
        "    def decode_token(self, _id):\n",
        "        \"\"\"\n",
        "        decodes token\n",
        "        :param _id: int or tuple\n",
        "        \"\"\"\n",
        "\n",
        "        def lookup(_id):\n",
        "            if type(_id) is int:\n",
        "                token = self.itos[_id]\n",
        "                if type(token) is str:\n",
        "                    return token\n",
        "                else:\n",
        "                    return lookup(token)\n",
        "            else: # _id is tuple\n",
        "                return lookup(_id[0]) + lookup(_id[1])\n",
        "\n",
        "        return lookup(_id)\n",
        "              \n",
        "    def decode(self, text):\n",
        "        \"\"\"\n",
        "        convert token ids into text\n",
        "        \"\"\"\n",
        "        return ''.join(map(self.decode_token, text))\n",
        "                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk2RkMERCZU7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8308c7be-458f-4144-a7d8-5a7234dcb1a2"
      },
      "source": [
        "from time import time\n",
        "s = time()\n",
        "vocab_size = 100\n",
        "bpe = BPE(vocab_size)\n",
        "tokenized_text = bpe.fit_transform(text)\n",
        "print(time() - s)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109.44263529777527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b96pFhV27-H6",
        "colab_type": "code",
        "outputId": "e923dc07-c30b-41ee-f290-433243a8f50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "assert bpe.decode(tokenized_text[0]) == text[0]\n",
        "print('OK')"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-7lZGiHJeR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def laplace_smoothing(n, d=1, tau=1):\n",
        "  \"\"\"\n",
        "  Counts Laplace smoothing\n",
        "\n",
        "  :param n: int\n",
        "  :param d: int or float, delta\n",
        "  tau: int or float\n",
        "  \"\"\"\n",
        "  return (n + d) ** (1 / tau)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTjH41UL7-H-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "        \n",
        "    \n",
        "class LM:\n",
        "    def __init__(self, vocab_size, delta=1):\n",
        "        self.delta = delta\n",
        "        self.vocab_size = vocab_size + 2\n",
        "        self.proba = None\n",
        "        self.start_token = vocab_size\n",
        "        self.end_token = vocab_size + 1\n",
        "        \n",
        "    def infer(self, a, b, tau=1):\n",
        "        \"\"\"\n",
        "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        result = [self.get_proba(a, b, i, tau) for i in range(self.vocab_size)]\n",
        "        return result\n",
        "        \n",
        "    def get_proba(self, a, b, c, tau=1):\n",
        "        \"\"\"\n",
        "        get probability of 3-gram (a,b,c)\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        c: third token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        probs = []\n",
        "\n",
        "        for i in range(self.vocab_size):\n",
        "            p = self.proba[(a, b, i)]\n",
        "            probs.append(laplace_smoothing(p))\n",
        "    \n",
        "        p_trigram = self.proba[(a, b, c)]\n",
        "        result = laplace_smoothing(p_trigram) / sum(probs)\n",
        "        return result\n",
        "    \n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        train language model on text\n",
        "        text: list of lists\n",
        "        \"\"\"\n",
        "        text = [[self.start_token] + sent + [self.end_token] for sent in text]\n",
        "        self.proba = count_ngrams(text, 3)\n",
        "        \n",
        "        return self\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj4MT80VJbkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm = LM(vocab_size, 1).fit(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFwLKRLVnED9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reform_results(l, k):\n",
        "    return sorted(enumerate(l), key=lambda x: x[1], reverse=True)[:k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPDGJ1gv7-IB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import log\n",
        "\n",
        "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
        "    \"\"\"\n",
        "    generate sequence from language model *lm* conditioned on input_seq\n",
        "    input_seq: sequence of token ids for conditioning\n",
        "    lm: language model\n",
        "    max_len: max generated sequence length\n",
        "    k: size of beam\n",
        "    tau: temperature\n",
        "    \"\"\"\n",
        "    if len(input_seq) < 2:\n",
        "        input_seq = [lm.start_token] + input_seq\n",
        "\n",
        "    a, b = input_seq[-2:]\n",
        "    most_probable = lm.infer(a, b, tau) \n",
        "    \n",
        "    beam = []\n",
        "    for token, proba in reform_results(most_probable, k):\n",
        "        beam.append(([b, token], log(proba))) # store in beam tuples of current sequences and their log probabilities\n",
        "    \n",
        "    for i in range(max_len):\n",
        "        candidates = []\n",
        "        candidates_proba = []\n",
        "    \n",
        "        for sent, sent_proba in beam:\n",
        "            if sent[-1] == lm.end_token:# TODO process terminal token \n",
        "                continue\n",
        "            else:\n",
        "                c, d = sent[-2:]\n",
        "                proba = lm.infer(c, d, tau)\n",
        "                best_k = reform_results(proba, k)\n",
        "                candidates += [sent + [token] for token, proba in best_k]\n",
        "                candidates_proba += [sent_proba + log(proba) for token, proba \n",
        "                                     in best_k]  \n",
        "      \n",
        "        beam = [(candidates[index], log_proba) for index, log_proba in reform_results(candidates_proba, k)]\n",
        "    return [(sent[1:], exp(sent_proba)) for sent, sent_proba in beam]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhKIEp8Tw_Sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_decoded(result, bpe, end):\n",
        "    for snt, snt_proba in result:\n",
        "        if snt[-1] == end:\n",
        "            snt = snt[:-1]\n",
        "        print(f'Сгенерировалось \"{bpe.decode(snt)}\"')\n",
        "        print(f'Вероятность: {snt_proba}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJmlFKQO7-ID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "cb77c54b-e422-42d1-874e-8fd9366cc777"
      },
      "source": [
        "input1 = 'horse '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "print_decoded(result, bpe, lm.end_token)"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Сгенерировалось \"with himself s\"\n",
            "Вероятность: 1.8281613740824351e-06\n",
            "Сгенерировалось \"with himself and \"\n",
            "Вероятность: 1.6490761782539504e-06\n",
            "Сгенерировалось \"with himself i\"\n",
            "Вероятность: 1.6117667624563522e-06\n",
            "Сгенерировалось \"with himself he \"\n",
            "Вероятность: 1.4625290992659495e-06\n",
            "Сгенерировалось \"with himself y\"\n",
            "Вероятность: 1.3506008518731458e-06\n",
            "Сгенерировалось \"with himself in\"\n",
            "Вероятность: 1.3506008518731458e-06\n",
            "Сгенерировалось \"with himself b\"\n",
            "Вероятность: 1.149130006566103e-06\n",
            "Сгенерировалось \"with himself a\"\n",
            "Вероятность: 1.0446636423328202e-06\n",
            "Сгенерировалось \"with himself c\"\n",
            "Вероятность: 9.84968577056659e-07\n",
            "Сгенерировалось \"with himself f\"\n",
            "Вероятность: 9.625829275780997e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGlhknm27-IG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "779704ad-4d37-4a8e-da7b-43011186e4e6"
      },
      "source": [
        "input1 = 'her'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "print_decoded(result, bpe, lm.end_token)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Сгенерировалось \"e with himself \"\n",
            "Вероятность: 1.3426008715224258e-05\n",
            "Сгенерировалось \"e with himself\"\n",
            "Вероятность: 2.60973939897802e-06\n",
            "Сгенерировалось \"e with himself w\"\n",
            "Вероятность: 1.6769409792629846e-06\n",
            "Сгенерировалось \"e with himselve\"\n",
            "Вероятность: 1.5403514317585013e-06\n",
            "Сгенерировалось \"e with his fack\"\n",
            "Вероятность: 1.2157350103734083e-06\n",
            "Сгенерировалось \"e with his face \"\n",
            "Вероятность: 1.176603817647416e-06\n",
            "Сгенерировалось \"e with himself to \"\n",
            "Вероятность: 1.037607230918971e-06\n",
            "Сгенерировалось \"e with his heas\"\n",
            "Вероятность: 9.673468884321173e-07\n",
            "Сгенерировалось \"e with his fact\"\n",
            "Вероятность: 9.667183293898269e-07\n",
            "Сгенерировалось \"e with his hear \"\n",
            "Вероятность: 8.910232584258684e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekaNE5Mq7-IJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "57db5166-14a0-4eec-b810-cfe2a5f0af18"
      },
      "source": [
        "input1 = 'what'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
        "print_decoded(result, bpe, lm.end_token)"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Сгенерировалось \"ed to himself you\"\n",
            "Вероятность: 3.9972382872566104e-06\n",
            "Сгенерировалось \"ed to himself in \"\n",
            "Вероятность: 1.9750752458246372e-06\n",
            "Сгенерировалось \"ed to himself it \"\n",
            "Вероятность: 1.7021541990555776e-06\n",
            "Сгенерировалось \"ed to himself i \"\n",
            "Вероятность: 1.4922512460114162e-06\n",
            "Сгенерировалось \"ed to himself be\"\n",
            "Вероятность: 9.330541012668513e-07\n",
            "Сгенерировалось \"ed to himself sa\"\n",
            "Вероятность: 9.303074810315902e-07\n",
            "Сгенерировалось \"ed to himself bu\"\n",
            "Вероятность: 8.981497051428168e-07\n",
            "Сгенерировалось \"ed to himself he ha\"\n",
            "Вероятность: 8.943145534674896e-07\n",
            "Сгенерировалось \"ed to himself he w\"\n",
            "Вероятность: 8.367494787615365e-07\n",
            "Сгенерировалось \"ed to himself is \"\n",
            "Вероятность: 8.060978953040398e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIO1ggb_7-IM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "04108821-5028-4fa7-b514-e8704a96b433"
      },
      "source": [
        "input1 = 'gun '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "print_decoded(result, bpe, lm.end_token)"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Сгенерировалось \"prince and their\"\n",
            "Вероятность: 1.6566138988035323e-06\n",
            "Сгенерировалось \"prince andressi\"\n",
            "Вероятность: 1.5657239895586834e-06\n",
            "Сгенерировалось \"prince with his \"\n",
            "Вероятность: 1.2917322251807314e-06\n",
            "Сгенерировалось \"prince andrew s\"\n",
            "Вероятность: 1.0067437466727315e-06\n",
            "Сгенерировалось \"prince andrew b\"\n",
            "Вероятность: 8.106248349832388e-07\n",
            "Сгенерировалось \"prince andrew and \"\n",
            "Вероятность: 7.714010526453404e-07\n",
            "Сгенерировалось \"prince and they w\"\n",
            "Вероятность: 7.202054718631012e-07\n",
            "Сгенерировалось \"prince andrew c\"\n",
            "Вероятность: 7.125653791384926e-07\n",
            "Сгенерировалось \"prince andrew h\"\n",
            "Вероятность: 7.060280820821751e-07\n",
            "Сгенерировалось \"prince andrew s \"\n",
            "Вероятность: 7.060280820821751e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qic_UPVn7-IR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f27b713-86fa-4499-febf-d4d53462d0d5"
      },
      "source": [
        "from math import exp, log\n",
        "\n",
        "def perplexity(snt, lm):\n",
        "    \"\"\"\n",
        "    snt: sequence of token ids\n",
        "    lm: language model\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(snt)\n",
        "    p = 0\n",
        "\n",
        "    for trigram in count_ngrams([snt], 3):\n",
        "      p += log(lm.get_proba(*trigram))\n",
        "    result = exp(p) ** (-1 / n) \n",
        "\n",
        "    return result\n",
        "\n",
        "perplexity(tokenized_text[0], lm)"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.989877698946975"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 254
        }
      ]
    }
  ]
}